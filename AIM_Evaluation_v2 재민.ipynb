{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec9a0213",
   "metadata": {},
   "source": [
    "# AIM: ì´ë¯¸ì§€ í‰ê°€ v2 - CLIP íˆíŠ¸ë§µ & ìº¡ì…˜\n",
    "- ì£¼ì œ(topic), í”„ë¡¬í”„íŠ¸(prompt), ì´ë¯¸ì§€ë“¤ì„ ì—…ë¡œë“œí•˜ê³  ê¸°ì¤€ ì´ë¯¸ì§€ ì„ íƒ\n",
    "- **NEW**: CLIP ì–´í…ì…˜ íˆíŠ¸ë§µìœ¼ë¡œ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœ ì‚¬ ì˜ì—­ ì‹œê°í™”\n",
    "- **NEW**: BLIP2ê°€ ìƒì„±í•œ ìº¡ì…˜ì„ ê²°ê³¼ í…Œì´ë¸”ì— í‘œì‹œ\n",
    "- Hugging Face í† í°ì€ `.env`ì˜ `HUGGING_FACE_API` ë˜ëŠ” `HF_TOKEN` ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a92fddf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT = /abr/co_show14/aim_eval\n"
     ]
    }
   ],
   "source": [
    "# í™˜ê²½ ì¤€ë¹„: í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€ ë° .env ë¡œë”©\n",
    "import os, sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "ROOT = os.path.dirname(os.path.dirname(os.getcwd())) if os.path.basename(os.getcwd())=='notebooks' else os.getcwd()\n",
    "if ROOT not in sys.path: sys.path.insert(0, ROOT)\n",
    "load_dotenv(find_dotenv())\n",
    "print('ROOT =', ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa45e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìœ„ì ¯ UI ë° ì¸ë„¤ì¼ í”„ë¦¬ë·°\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import io\n",
    "from PIL import Image as PILImage\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# ì…ë ¥ì°½ í¬ê¸° í™•ëŒ€\n",
    "topic_w = widgets.Textarea(\n",
    "    value='A wooden cabin with a smoking chimney stands among snow-covered trees and a snowy mountain in the back.',\n",
    "    description='Topic',\n",
    "    style={'description_width':'initial'},\n",
    "    layout=widgets.Layout(width='800px', height='80px')\n",
    ")\n",
    "\n",
    "prompt_w = widgets.Textarea(\n",
    "    value='A cozy wooden cabin with a smoking chimney in a snowy forest, majestic snow-covered mountain in the background, crisp winter day, cinematic lighting, hyperrealistic, 8k',\n",
    "    description='Prompt',\n",
    "    style={'description_width':'initial'},\n",
    "    layout=widgets.Layout(width='800px', height='80px')\n",
    ")\n",
    "\n",
    "files_w = widgets.FileUpload(accept='image/*', multiple=True, description='Upload Images')\n",
    "ref_dropdown = widgets.Dropdown(options=[], description='Reference Image', style={'description_width':'initial'})\n",
    "enable_clip_w = widgets.Checkbox(value=True, description='Enable CLIP')\n",
    "enable_blip2_w = widgets.Checkbox(value=True, description='Enable BLIP2')\n",
    "enable_lpips_w = widgets.Checkbox(value=True, description='Enable LPIPS')\n",
    "run_btn = widgets.Button(description='Run Evaluation', button_style='primary')\n",
    "thumbs_out = widgets.Output()\n",
    "out = widgets.Output()\n",
    "heatmap_out = widgets.Output()  # CLIP íˆíŠ¸ë§µ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "visualization_helpers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP heatmap functions ready\n"
     ]
    }
   ],
   "source": [
    "# CLIP íˆíŠ¸ë§µ ìƒì„± í•¨ìˆ˜\n",
    "import torch\n",
    "\n",
    "def create_clip_heatmap(eval_inst, image_path, prompt):\n",
    "    \"\"\"CLIP ì–´í…ì…˜ ë§µì„ ìƒì„±í•˜ì—¬ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœ ì‚¬ ì˜ì—­ì„ ì‹œê°í™”\"\"\"\n",
    "    if not eval_inst.enable_clip:\n",
    "        return None\n",
    "        \n",
    "    from PIL import Image\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    inputs = eval_inst.clip_processor(\n",
    "        text=[prompt],\n",
    "        images=image,\n",
    "        return_tensors='pt',\n",
    "        padding=True\n",
    "    )\n",
    "    inputs = {k: v.to(eval_inst.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # CLIP ë¹„ì „ ì¸ì½”ë”ì—ì„œ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì¶”ì¶œ\n",
    "        vision_outputs = eval_inst.clip_model.vision_model(**{'pixel_values': inputs['pixel_values']}, output_attentions=True)\n",
    "        text_outputs = eval_inst.clip_model.text_model(**{'input_ids': inputs['input_ids'], 'attention_mask': inputs.get('attention_mask')})\n",
    "        \n",
    "        # ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‚¬ìš©\n",
    "        attention_weights = vision_outputs.attentions[-1]  # [batch, heads, seq_len, seq_len]\n",
    "        \n",
    "        # CLS í† í°(ì²« ë²ˆì§¸)ì— ëŒ€í•œ ì–´í…ì…˜ì„ í‰ê· í™”\n",
    "        cls_attention = attention_weights[0, :, 0, 1:].mean(0)  # [patches]\n",
    "        \n",
    "        # íŒ¨ì¹˜ ê·¸ë¦¬ë“œë¡œ ì¬êµ¬ì„± (ViT-B/32ëŠ” 7x7 íŒ¨ì¹˜)\n",
    "        patch_size = int(cls_attention.shape[0] ** 0.5)\n",
    "        attention_map = cls_attention.reshape(patch_size, patch_size)\n",
    "        attention_map = attention_map.cpu().numpy()\n",
    "        \n",
    "        # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ì—…ìŠ¤ì¼€ì¼\n",
    "        attention_resized = cv2.resize(attention_map, image.size, interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        return image, attention_resized\n",
    "\n",
    "def display_clip_heatmap(image, attention_map, title):\n",
    "    \"\"\"CLIP attention heatmap visualization\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Attention map\n",
    "    im = axes[1].imshow(attention_map, cmap='hot', alpha=0.8)\n",
    "    axes[1].set_title('CLIP Attention Map')\n",
    "    axes[1].axis('off')\n",
    "    plt.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Overlay\n",
    "    axes[2].imshow(image)\n",
    "    axes[2].imshow(attention_map, cmap='hot', alpha=0.4)\n",
    "    axes[2].set_title('Overlay (Focus Areas)')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'{title} - CLIP Attention Analysis', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print('CLIP heatmap functions ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac4f88c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for v2 evaluation.\n"
     ]
    }
   ],
   "source": [
    "# í‰ê°€ ì‹¤í–‰ ë¡œì§\n",
    "import tempfile, io\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import evaluation\n",
    "\n",
    "def run_eval_clicked(_):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        if not files_w.value:\n",
    "            print('ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.')\n",
    "            return\n",
    "        \n",
    "        # ì§„í–‰ ìƒíƒœ í‘œì‹œ\n",
    "        print(\"ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "        \n",
    "        # í™˜ê²½ í”Œë˜ê·¸ ì„¤ì •\n",
    "        os.environ['DISABLE_CLIP'] = '0' if enable_clip_w.value else '1'\n",
    "        os.environ['DISABLE_BLIP2'] = '0' if enable_blip2_w.value else '1'\n",
    "        os.environ['DISABLE_LPIPS'] = '0' if enable_lpips_w.value else '1'\n",
    "        \n",
    "        # í‰ê°€ê¸° ì¸ìŠ¤í„´ìŠ¤\n",
    "        eval_inst = evaluation.get_evaluator()\n",
    "        \n",
    "        print(\"ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        # ì—…ë¡œë“œ íŒŒì¼ì„ ì„ì‹œê²½ë¡œì— ì €ì¥\n",
    "        tmpdir = tempfile.mkdtemp(prefix='aim_nb_v2_')\n",
    "        name_to_path = {}\n",
    "        for i, item in enumerate(files_w.value):\n",
    "            if hasattr(item, 'name') and item.name:\n",
    "                name = item.name\n",
    "            else:\n",
    "                meta = item.get('metadata') or {}\n",
    "                name = meta.get('name', f'image_{i+1}.png')\n",
    "            \n",
    "            content = item['content'] if isinstance(item['content'], (bytes, bytearray)) else item['content'].tobytes()\n",
    "            img = Image.open(io.BytesIO(content)).convert('RGB')\n",
    "            path = os.path.join(tmpdir, name)\n",
    "            img.save(path)\n",
    "            name_to_path[name] = path\n",
    "            \n",
    "        ref_name = ref_dropdown.value\n",
    "        if ref_name not in name_to_path:\n",
    "            print('ê¸°ì¤€ ì´ë¯¸ì§€ê°€ ëª©ë¡ì— ì—†ìŠµë‹ˆë‹¤.')\n",
    "            return\n",
    "            \n",
    "        print(\"ì´ë¯¸ì§€ í‰ê°€ ì¤‘...\")\n",
    "        \n",
    "        topic = topic_w.value\n",
    "        prompt = prompt_w.value\n",
    "        results = []\n",
    "        captions = {}  # ìº¡ì…˜ì„ ë³„ë„ë¡œ ì €ì¥\n",
    "        \n",
    "        # ê¸°ì¤€ ì´ë¯¸ì§€ëŠ” LPIPSë¥¼ ì œì™¸í•˜ê³  í‰ê°€\n",
    "        for name, path in name_to_path.items():\n",
    "            if name == ref_name:\n",
    "                # ê¸°ì¤€ ì´ë¯¸ì§€ëŠ” LPIPS ì œì™¸\n",
    "                r = eval_inst.evaluate(path, name_to_path[ref_name], prompt, topic)\n",
    "                r['lpips_score'] = None\n",
    "                r['lpips_norm'] = None\n",
    "                # ê¸°ì¤€ ì´ë¯¸ì§€ëŠ” LPIPS ì œì™¸í•˜ê³  final_score ì¬ê³„ì‚°\n",
    "                clip_norm = r['clip_norm']\n",
    "                blip2_norm = r['blip2_norm']\n",
    "                weights = {\n",
    "                    'clip': 0.2 if enable_clip_w.value else 0.0,\n",
    "                    'blip2': 0.5 if enable_blip2_w.value else 0.0,\n",
    "                }\n",
    "                weight_sum = sum(weights.values()) or 1.0\n",
    "                score = (clip_norm * weights['clip'] + blip2_norm * weights['blip2'])\n",
    "                r['final_score'] = (score / weight_sum) * 100\n",
    "                r['is_reference'] = True\n",
    "            else:\n",
    "                # ë‹¤ë¥¸ ì´ë¯¸ì§€ëŠ” ì „ì²´ í‰ê°€\n",
    "                r = eval_inst.evaluate(path, name_to_path[ref_name], prompt, topic)\n",
    "                r['is_reference'] = False\n",
    "            \n",
    "            # ìƒì„±ëœ ìº¡ì…˜ì„ ë³„ë„ë¡œ ì €ì¥ (BLIP2ê°€ í™œì„±í™”ëœ ê²½ìš°)\n",
    "            if enable_blip2_w.value and hasattr(eval_inst, '_caption_cache'):\n",
    "                captions[name] = eval_inst._caption_cache.get(path, 'ìº¡ì…˜ ìƒì„± ì‹¤íŒ¨')\n",
    "            \n",
    "            r['filename'] = name\n",
    "            results.append(r)\n",
    "            \n",
    "        if not results:\n",
    "            print('í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.')\n",
    "            return\n",
    "            \n",
    "        print(\"í‰ê°€ ì™„ë£Œ\")\n",
    "        \n",
    "        # ìº¡ì…˜ì„ ì œì™¸í•œ ê²°ê³¼ í…Œì´ë¸” í‘œì‹œ\n",
    "        columns = ['filename', 'clip_score', 'clip_norm', 'blip2_similarity', 'blip2_norm', 'lpips_score', 'lpips_norm', 'final_score']\n",
    "        df = pd.DataFrame(results)[columns]\n",
    "        display(df)\n",
    "        \n",
    "        # ìƒì„±ëœ ìº¡ì…˜ì„ ë³„ë„ë¡œ í‘œì‹œ\n",
    "        if enable_blip2_w.value and captions:\n",
    "            print(\"\\n=== Generated Captions ===\")\n",
    "            for name, caption in captions.items():\n",
    "                print(f\"{name}: {caption}\")\n",
    "        \n",
    "        # CLIP íˆíŠ¸ë§µ ìƒì„± ë° í‘œì‹œ\n",
    "        if enable_clip_w.value:\n",
    "            print(\"CLIP ì–´í…ì…˜ íˆíŠ¸ë§µ ìƒì„± ì¤‘...\")\n",
    "            with heatmap_out:\n",
    "                clear_output()\n",
    "                for result in results:\n",
    "                    name = result['filename']\n",
    "                    path = name_to_path[name]\n",
    "                    try:\n",
    "                        heatmap_result = create_clip_heatmap(eval_inst, path, prompt)\n",
    "                        if heatmap_result:\n",
    "                            image, attention_map = heatmap_result\n",
    "                            display_clip_heatmap(image, attention_map, name)\n",
    "                    except Exception as e:\n",
    "                        print(f\"{name} CLIP íˆíŠ¸ë§µ ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "            print(\"CLIP ì–´í…ì…˜ íˆíŠ¸ë§µ ì™„ë£Œ!\")\n",
    "       \n",
    "run_btn.on_click(run_eval_clicked)\n",
    "print('Ready for v2 evaluation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8029f7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7f8333cb25e43a086f5465d7371d17f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='A wooden cabin with a smoking chimney stands among snow-covered trees and a snowy mountain in â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c13af61c86d4127806c05bfc42c89cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='A cozy wooden cabin with a smoking chimney in a snowy forest, majestic snow-covered mountain iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ad5b9083ed4c5b9eab8c4040e94c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='image/*', description='Upload Images', multiple=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238881826b324757b7692cd1b964d5cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Reference Image', options=(), style=DescriptionStyle(description_width='initial'), valueâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae782333fdd413cbc60b3b1d1c0fecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Checkbox(value=True, description='Enable CLIP'), Checkbox(value=True, description='Enable BLIP2â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aef85abf2dc43ae9f7c12370a4477dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7373b5c8868a4bf5ae4cdcc9d2146349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Run Evaluation', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f412e0f1b07440f285c56f7503868588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value=\"\\n    <div style='background-color: #f8f9fa; padding: 15px; border-radius: 8px; border: 1px solid â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88740688f8034b1681bb5858beff6712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09632b3fd80a4784a87ca12610cd58f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value=\"<h2 style='color:#1976d2; margin-top:30px;'>ğŸ”¥ CLIP ì–´í…ì…˜ íˆíŠ¸ë§µ</h2>\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b5e6831ebd48818c7f9137bbe7977f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def render_thumbnails(change=None):\n",
    "    with thumbs_out:\n",
    "        clear_output()\n",
    "        if not files_w.value:\n",
    "            print('ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.')\n",
    "            return\n",
    "        items = []\n",
    "        current_ref = ref_dropdown.value or ''\n",
    "        for i, f in enumerate(files_w.value):\n",
    "            # íŒŒì¼ëª… ì¶”ì¶œ ê°œì„ \n",
    "            if hasattr(f, 'name') and f.name:\n",
    "                name = f.name\n",
    "            else:\n",
    "                meta = f.get('metadata') or {}\n",
    "                name = meta.get('name', f'image_{i+1}.png')\n",
    "            \n",
    "            content = f['content'] if isinstance(f['content'], (bytes, bytearray)) else f['content'].tobytes()\n",
    "            img = PILImage.open(io.BytesIO(content)).convert('RGB')\n",
    "            thumb = img.copy()\n",
    "            thumb.thumbnail((160,160))\n",
    "            buf = io.BytesIO()\n",
    "            thumb.save(buf, format='PNG')\n",
    "            wimg = widgets.Image(value=buf.getvalue(), format='png', width=160, height=160)\n",
    "            border = '3px solid #0056b3' if name == current_ref else '1px solid #ccc'\n",
    "            box = widgets.VBox([wimg, widgets.HTML(f\"<div style='text-align:center; font-size:12px'>{name}</div>\")],\n",
    "                               layout=widgets.Layout(border=border, padding='4px', margin='4px', align_items='center', width='180px'))\n",
    "            items.append(box)\n",
    "        grid = widgets.GridBox(children=items, layout=widgets.Layout(grid_template_columns='repeat(4, 180px)'))\n",
    "        display(grid)\n",
    "\n",
    "def update_ref_options(change=None):\n",
    "    names = []\n",
    "    for i, f in enumerate(files_w.value):\n",
    "        # íŒŒì¼ëª… ì¶”ì¶œ ê°œì„ \n",
    "        if hasattr(f, 'name') and f.name:\n",
    "            name = f.name\n",
    "        else:\n",
    "            meta = f.get('metadata') or {}\n",
    "            name = meta.get('name', f'image_{i+1}.png')\n",
    "        names.append(name)\n",
    "    \n",
    "    ref_dropdown.options = names\n",
    "    if names: \n",
    "        ref_dropdown.value = names[0]\n",
    "    render_thumbnails()\n",
    "\n",
    "files_w.observe(update_ref_options, names='value')\n",
    "ref_dropdown.observe(render_thumbnails, names='value')\n",
    "\n",
    "# í‰ê°€ ë°©ì‹ ì„¤ëª… ì—…ë°ì´íŠ¸ (v2)\n",
    "info_panel = widgets.HTML(\n",
    "    value=\"\"\"\n",
    "    <div style='background-color: #f8f9fa; padding: 15px; border-radius: 8px; border: 1px solid #dee2e6; margin-top: 10px;'>\n",
    "        <h4 style='margin-top: 0; color: #495057;'>ğŸ“Š AIM v2 í‰ê°€ ë°©ì‹ ì„¤ëª…</h4>\n",
    "        <ul style='margin-bottom: 0; color: #6c757d; line-height: 1.6;'>\n",
    "            <li><strong>ğŸ”— CLIP ì ìˆ˜:</strong> í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìœ ì‚¬ë„ (0.4 ì´ìƒ â†’ 1.0, ì•„ë‹ˆë©´ (score+1)/1.4)</li>\n",
    "            <li><strong>ğŸ’¬ BLIP2 ì ìˆ˜:</strong> í† í”½ ìœ ì‚¬ë„ (0.7 ì´ìƒ â†’ 1.0, ì•„ë‹ˆë©´ score/0.7)</li>\n",
    "            <li><strong>ğŸ‘ï¸ LPIPS ì ìˆ˜:</strong> ì´ë¯¸ì§€ ì‹œê°ì  ìœ ì‚¬ë„ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ, 1-scoreë¡œ ì •ê·œí™”)</li>\n",
    "            <li><strong>ğŸ† Final Score:</strong> ê°€ì¤‘í‰ê·  (CLIP:20%, BLIP2:50%, LPIPS:30%) Ã— 100</li>\n",
    "        </ul>\n",
    "        <div style='background-color: #e3f2fd; padding: 10px; border-radius: 6px; margin-top: 10px;'>\n",
    "            <strong>ğŸ†• v2 ìƒˆë¡œìš´ ê¸°ëŠ¥:</strong><br>\n",
    "            â€¢ í…Œì´ë¸”ì— ìƒì„±ëœ ìº¡ì…˜ í‘œì‹œ<br>  \n",
    "            â€¢ CLIP ì–´í…ì…˜ íˆíŠ¸ë§µìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ì™€ ìœ ì‚¬í•œ ì´ë¯¸ì§€ ì˜ì—­ ì‹œê°í™”<br>\n",
    "            â€¢ ë¹¨ê°„ìƒ‰ì¼ìˆ˜ë¡ í”„ë¡¬í”„íŠ¸ì™€ ìœ ì‚¬í•˜ë‹¤ê³  CLIPì´ íŒë‹¨í•œ ì˜ì—­\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\",\n",
    "    layout=widgets.Layout(width='800px')\n",
    ")\n",
    "\n",
    "# UI í‘œì‹œ\n",
    "display(\n",
    "    topic_w, \n",
    "    prompt_w, \n",
    "    files_w, \n",
    "    ref_dropdown, \n",
    "    widgets.HBox([enable_clip_w, enable_blip2_w, enable_lpips_w]), \n",
    "    thumbs_out, \n",
    "    run_btn, \n",
    "    info_panel,\n",
    "    out,\n",
    "    widgets.HTML(\"<h2 style='color:#1976d2; margin-top:30px;'>ğŸ”¥ CLIP ì–´í…ì…˜ íˆíŠ¸ë§µ</h2>\"),\n",
    "    heatmap_out  # CLIP íˆíŠ¸ë§µ ì¶œë ¥ ì˜ì—­\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
